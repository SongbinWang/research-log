# 2026.1.8

**今日进展**

- 重新学习[Transformer](###Transformer再学习)，读deit vit的代码

- 重新读BIP的论文，调试代码回顾流程

主要是对之前进展的回顾











### Transformer再学习

开始: encoder-decoder架构, seq2seq条件生成, 目的是机器翻译

BERT: encoder, 目的是进行判别或表示, 不擅长生成

LLM: decoder-only, 使用language modeling(自回归), 通过Prompt, 对下一个token进行预测, 目的变成了语言生成

​	llm在逐字生成的过程, 就是不断在对下一个词进行**预测**和**选取**的过程。



token: 对于文本->是每个词每个标点符号甚至是词的字母组合字段

​	     对于图像->图像被划分为多个图像块, 每个图像块是一个token

​	     对于音频->token是每一个音频片段

(模型权重会不断更新去拟合数据，而数据产生的向量在经过模型预测过程中也会不断更新来获取结果)

每个token会对应一个向量, 理解为高维空间的向量, 具有相似表意的向量会在空间上分布更加靠近。

一组data会被初始化为一组tokens, 然后经过Attention模块，通过上下文来准确判断不断更新向量的值，使得tokens获得到词所表达的丰富含义(被向量内的一组数字所表示)，然后再经过MLP(多层感知层)并行更新向量。

不断更新的过程, 实际就是不断进行的矩阵运算。重复经过**多组attention-MLP**之后，核心含义被融入到**最后一个向量**中，对这个向量进行操作，产生覆盖所有可能token的**概率分布**，然后进行预测。

- embedding

  模型会有一个预设的词库，由一个embedding matrix($W_e$)(嵌入矩阵)表示, 每个列就是每个词的向量表示。定义了每个token应该被转化成的向量。列数为token的数量，行数为嵌入维度。(维度 X token数) = **产生的参数量**

  这个嵌入矩阵会被随机初始化，在学习过程中基于data**不断调整更新**。

  在直觉上，两个词的表意差距可以模型化为两个向量的距离。(因此在一些词上加上另一个词的向量后会得到两个词意叠加的词)。

  - 两个向量之间的点积 可以被视为 一种衡量它们是否对齐的方法。两个向量垂直为0，方向相似为正数，相反为负数。

  context size(上下文大小): 模型每次能够处理的最大向量的数量(最多token的数量)，所以一次处理的就是context size列数的向量组成的数组

- unembedding

  经过模型后产生的最后一个向量，使用一个unembedding矩阵($W_u$)(全连接层)将其映射到与词库大小相同的列表中，每个token对应列表中的一个值，这个列表(或者向量)叫做**logits**，然后经过softmax将其转换为概率分布。(token数 X 维度) = **产生的参数量**

  softmax的输入值，即与unembedding矩阵相乘后得到的为标准化的输出成分，称为下一个词的**logits** ， 输出值为**probabilities**

  - 在softmax中，通常还会加上一个温度参数t, 当t越大时候, 较小的数值会获得更多的权重, 以增加随机性，但是一般这个温度参数不会调的特别高。

  unembedding矩阵同样是随机初始化，在学习过程中基于data**不断调整更新**。

- Attention(self-attention)

  $Attention(Q,K,V)=softmax(\frac{K^TQ}{\sqrt{d_k}})V$

  一开始token所对应的向量仅仅是embedding后获得一定初始语义以及位置编码的向量($\vec E_n$)，在输入中经过embedding的同一个词的向量数字相同，在经过Attention更新调整后，向量的方向会产生移动，然后获得联系上下文的语义。

  - Query

    一个用于查询上下文相关词的查询向量，通过矩阵$W_Q$(模型的参数)和嵌入向量运算后获得，将embedding向量映射到了较小的维度空间。

  - Key

    通过矩阵$W_K$(模型的参数)和嵌入向量运算后获得，是对查询向量$\vec Q$来说潜在的查询回答者。

  - Value

    矩阵$W_V$和每一个嵌入向量$\vec E_n$运算获得与嵌入维度相当的高维向量$\vec V_n$, 与attention pattern计算，用于更新embedding向量

    $W_V$矩阵实际上是由一个降维和升维矩阵构成的，在实际的多头注意力应用中，往往embedding向量只与降维矩阵做运算，然后每个注意力头的升维矩阵合并成了一个大的输出矩阵(output matrix)，最后在与这个输出投影矩阵做乘法运算，得到输出。

  K与Q向量之间的对齐程度可视作，K是否是Q的准确回答，因此对K和Q进行点积运算，就可以得到两者相似度的矩阵，点积数值大的部分即"注意力集中到了"。对每一列进行softmax，就获得了"attention pattern(注意力模式)"。除以$d_k$是为了保证数值的稳定性。

  技术细节：对给定文本进行处理过程中，往往还会对这段文本的子序列的下一个token都进行预测，能够提高训练的效率。

  attention pattern的大小为context size的平方，因此上下文大小经常是Transformer架构下的瓶颈。

  (cross-attention交叉注意力机制，与自注意不同的是K和Q是来自于不同的数据，并且无需做掩码。)


  多头注意力，对于每一个注意力头都会有自己的$W_{K}^{(n)},W_{Q}^{(n)},W_{V}^{(n)}$，每个注意力头都产生不同的注意力，将每一个注意力头产生的向量都叠加到embedding向量中，获得输。

- masking

  注意力机制的一个原则是不能够看到后面的词汇，使用掩码将后面的词给遮住，具体做法是将KQ运算后的矩阵的下三角部分设置为负无穷，这样在经过softmax之后它们会被削减为0。(不是所有的应用场景都会使用掩码)

- MLP(多层感知器)

  包含两个线性层和一个非线性函数。