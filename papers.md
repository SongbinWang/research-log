**[1] AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition,** NeurIPS 2022.

*Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, Ping Luo.*

[[Paper](https://arxiv.org/abs/2205.13535)]  [[Code](https://github.com/ShoufaChen/AdaptFormer)]

**[2] Convolutional Bypasses are Better Vision Transformer Adapters,** Arxiv 2022.

*Jie, Shibo and Deng, Zhi-Hong.*

[[Paper](https://arxiv.org/abs/2207.07039)]  [[Code](https://github.com/JieShibo/PETL-ViT)] 

**[6] 1% VS 100%: Parameter-Efficient Low Rank Adapter for Dense Predictions,** CVPR 2023.

  *Yin, Dongshuo and Yang, Yiran and Wang, Zhechao and Yu, Hongfeng and Wei, Kaiwen and Sun, Xian.*

  [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Yin_1_VS_100_Parameter-Efficient_Low_Rank_Adapter_for_Dense_Predictions_CVPR_2023_paper.html)] [Code] 

**[9] SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient Channels,** IJCV 2023.

  *Henry Hengyuan Zhao, Pichao Wang, Yuyang Zhao, Hao Luo, Fan Wang, Mike Zheng Shou.*

  [[Paper](https://arxiv.org/abs/2303.07910)]  [[Code](https://github.com/showlab/SCT)]

**[12] Compacter: Efficient Low-Rank Hypercomplex Adapter Layer,** NeurIPS 2021.

  *Karimi Mahabadi, Rabeeh and Henderson, James and Ruder, Sebastian.*

  [[Paper](https://arxiv.org/abs/2106.04647)] [[Code](https://github.com/rabeehk/compacter)]

**[13] Parameter-efficient and student-friendly knowledge distillation,** NeurIPS 2022.

  *Rao, Jun and Meng, Xv and Ding, Liang and Qi, Shuhan and Tao, Dacheng.*

  [[Paper](https://arxiv.org/abs/2205.15308)]  [Code]



**[1] Visual Prompt Tuning,** ECCV 2022.

  *Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, Ser-Nam Lim.*

  [[Paper](https://arxiv.org/abs/2203.12119)]  [[Code](https://github.com/kmnp/vpt)]

**[2] Visual Prompt Tuning for Test-time Domain Adaptation,** Arxiv 2022.

  *Gao, Yunhe and Shi, Xingjian and Zhu, Yi and Wang, Hao and Tang, Zhiqiang and Zhou, Xiong and others.*

  [[Paper](https://arxiv.org/pdf/2210.04831.pdf)]  [Code]

**[3] LPT: Long-tailed Prompt Tuning for Image Classification,** ICLR 2023.

  *Dong, Bowen and Zhou, Pan and Yan, Shuicheng and Zuo, Wangmeng.*

  [[Paper](https://arxiv.org/abs/2210.01033)]  [[Code](https://github.com/DongSky/LPT)]

**[5] Instance-aware Dynamic Prompt Tuning for Pre-trained Point Cloud Models,** ICCV 2023.

  *Zha, Yaohua and Wang, Jinpeng and Dai, Tao and Chen, Bin and Wang, Zhi and Xia, Shu-Tao.*

  [[Paper](https://arxiv.org/pdf/2304.07221.pdf)]  [[Code](https://github.com/zyh16143998882/ICCV23-IDPT)]

**[7] LION: Implicit Vision Prompt Tuning,** AAAI 2024.

  *Wang, Haixin and Chang, Jianlong and Luo, Xiao and Sun, Jinan and Lin, Zhouchen and Tian, Qi.*

  [[Paper](https://arxiv.org/abs/2303.09992)]  [Code]



- `CoOp` **Learning to Prompt for Vision-Language Models.** IJCV 2022. 
  [[Paper](https://arxiv.org/abs/2203.05557)] [[Code](https://github.com/KaiyangZhou/CoOp)] 
  
- `CoCoOp` **Conditional Prompt Learning for Vision-Language Models.** CVPR 2022. 
  [[Paper](https://arxiv.org/abs/2203.05557)] [[Code](https://github.com/KaiyangZhou/CoOp)] 

- `MaPLe` **MaPLe: Multi-modal Prompt Learning.** CVPR 2023.  

  [[Paper](https://arxiv.org/abs/2210.03117)] [[Code](https://github.com/muzairkhattak/multimodal-prompt-learning)]

- `KgCoOp` **Visual-Language Prompt Tuningx with Knowledge-guided Context Optimization.** CVPR 2023. 
  [[Paper](https://arxiv.org/abs/2303.13283)] [[Code](https://github.com/htyao89/KgCoOp)]

- `RPO` **Read-only Prompt Optimization for Vision-Language Few-shot Learning.** ICCV 2023. 
  [[Paper](https://arxiv.org/abs/2308.14960)] [[Code](https://github.com/mlvlab/rpo)] 

- `CuPL` **What does a platypus look like? Generating customized prompts for zero-shot image classification.** ICCV 2023. 
  [[Paper](https://arxiv.org/pdf/2209.03320)] [[Code](https://github.com/sarahpratt/CuPL)] 

- `PromptSRC` **Self-regulating Prompts: Foundational Model Adaptation without Forgetting.** ICCV 2023. 
  [[Paper](https://arxiv.org/abs/2307.06948)] [[Code](https://github.com/muzairkhattak/PromptSRC)] 

- `PLOT` **PLOT: Prompt Learning with Optimal Transport for Vision-Language Models.** ICLR 2023. 
  [[Paper](https://arxiv.org/pdf/2210.01253)] [[Code](https://github.com/CHENGY12/PLOT)] 

- `CoPrompt` **Consistency-guided Prompt Learning for Vision-Language Models.** ICLR 2024. 
  [[Paper](https://arxiv.org/abs/2306.01195)] [[Code](https://github.com/ShuvenduRoy/CoPrompt)]

- `PromptKD` **PromptKD: Unsupervised Prompt Distillation for Vision Language Models.** CVPR 2024.  [[Paper](https://arxiv.org/abs/2403.02781)] [[Code](https://github.com/zhengli97/PromptKD)]  
- `DePT` **DePT: Decoupled Prompt Tuning.** CVPR 2024. 
[[Paper](https://arxiv.org/abs/2309.07439)] [[Code](https://github.com/Koorye/DePT)]
- `ArGue` **ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models.** CVPR 2024. 
[[Paper](https://arxiv.org/abs/2311.16494)] [No Code Found]
- `TextRefiner` **TextRefiner: Internal Visual Feature as Efficient Refiner for Vision-Language Models Prompt Tuning.** AAAI 2025. 
[[Paper](https://arxiv.org/abs/2412.08176)] [[Code](https://github.com/xjjxmu/TextRefiner)]
- `TCP` **TCP: Textual-based Class-aware Prompt tuning for Visual-Language Model.** CVPR 2024.
[[Paper](https://arxiv.org/abs/2311.18231)] [[Code](https://github.com/htyao89/Textual-based_Class-aware_prompt_tuning)]



**Prompt-based Adaptation in Large-scale Vision Models: A Survey**. [[paper]](https://github.com/yunbeizhang/Awesome-Visual-Prompt-Tuning?tab=readme-ov-file)

