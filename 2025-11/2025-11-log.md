# 2025.11.2

**今日进展**

- 阅读文献：[**PLOT: Prompt Learning with Optimal Transport for Vision-Language Models.**](###PLOT)

  PLOT主要对多个文本Prompt特征与图像特征进行对齐的问题进行优化，该方法思路为：将文本与图像特征的余弦相似度计算 **优化为** 最优通道算法，以此提点，但是同时降低了训练和推理的效率。

  **可能改进点**：针对图文相似度拉近的算法继续进行优化；作为新方法用来提点的一个组件来使用

  

- 继续对[CoPrompt](###CoPrompt)的源代码进行阅读学习，此前已看完总体框架的构建，今日主要针对训练的过程。

  主要针对其与[maple](###MaPLe)方法的不同点：

  1. 冻结模型蒸馏指导可学习模型的模块：冻结模块的输入为扰动输入，具体查看了**数据是如何处理的**
  2. Adapter模块：Adapter模块的加入很容易导致过拟合，代码中**使用一个置信度超参数进行了平衡(在论文中并没有提及)**
  3. LLM生成增强文本：并非将所有Prompt的embedding进行平均，而是在训练过程**动态随机选择某一条作为扰动输入**



**下阶段目标**：针对PromptSRC的源代码进行阅读学习，分析PromptSRC中的代码设计思路，PromptSRC与CoPrompt的差异点。继续阅读相关论文



# 2025.11.4

**今日进展**

- 看[PromptSRC](###PromptSRC)的源代码, 对于训练过程，从总体框架到细节的具体实现, 重点是与先前已经看过的代码项目的不同之处

  1. 将包含学习参数的编码器与冻结的编码器分开, **采用知识蒸馏的思想**, 用冻结的去指导包含可学习的, 具体实现即对其做L1范数损失调节
  2. 不同于[MaPLe](###MaPLe)在设计Prompt learner类时候，在其中设计好VPTdeep，而是**在clip.py中的注意力模块部分实现VPTshallow**

  但忽略了自集成模块的实现方式。

  发现之前读的maple源代码中**被忽略的部分细节**：主要来自于clip.py文件中maple方法引入在attention层的跨模态交互部分



**下阶段目标**：查看高斯权重集成模块的具体实现，读PromptSRC的代码的测试阶段部分, 回头看maple方法之前被忽略的细节。



# 2025.11.6

**今日进展**

- [**Prompt-based Adaptation in Large-scale Vision Models: A Survey**](###PAmethod) 针对此前阅读的这篇有关视觉模型Prompt方法综述的文章进行总结, 将相关的新论文拉表汇总: 主要是 2025年pub的 PA相关文章；现阶段学习有关的CLIP或者多模态语言模型的文章 
- 查看了[PromptSRC](###PromptSRC)的权重自集成模块的具体实现



**下阶段目标**：看完PromptSRC的测试部分, 结束PromptSRC代码的阅读。进行下一个论文`TCP`和`BIP`的阅读。



# 2025.11.9

**今日进展**

- 阅读论文：[TCP:Textual-based Class-aware Prompt tuning for Visual-Language Model](###TCP) 以及TCP的代码。

  TCP在KgCoOp的基础上改进，不仅将可学习Prompt与原本手工设计Prompt做相似度损失，而且**经过一个类似Adapter模块(TEK)**微调后**将其插入到中间层**进一步引导Prompt learner的优化

- 代码实现的**核心模块**在于：将class-aware Prompt插入到文本编码器中间层，这个过程关键设计是 :

  1.class-aware文本特征经过TEK之后, 需要进行`reshape(class_feature.shape[0], -1, 512)`确保形状与Prompt learner一致同时维度和文本编码器能对齐(512) 

  2.`self.transformer.resblocks()`的改动, 使其能够适应list输入, 实现在特定transformer层中插入Prompt



**下阶段目标**：看完PromptSRC的测试部分, 结束PromptSRC代码的阅读。继续深入学习TCP的代码



# 2025.11.10

**今日进展**

- 读[PromptSRC](###PromptSRC)代码的测试部分：推理阶段只需要经过原本的编码器结构，将图文进行相似度计算即可，因此损耗只在于Prompt的tokens计算

  在transformer层插入Prompt learner的设计也与单纯的deep-VPT不同，deep-VPT是叠加式，上层的Prompt会留在下一层的序列中；而PromptSRC中则是把上层的Prompt替换成新的Prompt独立学习
  
  将residualAttentionBlock部分与maple作对比：逻辑一致，在实现上存在细微区别

- 跑TCP的部分实验



**下阶段目标**：继续学习TCP的代码部分。阅读BIP论文



# 2025.11.11

**今日进展**

- 继续读[TCP](###TCP)的代码，仔细研究class-aware prompt的生成过程和插入机制

  class-aware prompt插入机制基于maple的代码设计的residualAttentionBlock修改而成，添加一个`counter`作为索引，在transformer层传递过程中递增，到达指定层数后插入该Prompt。

- 通过读TCP的代码，在插入class-aware Prompt的操作中，代码权重设置为1，也就是对learnable Prompt进行了完全替换，由此发现之前对于transformer和Prompt tuning的理解太过片面：

  **关于transformer的注意力机制和Prompt的影响**

  transformer模型由多个transformer层堆叠而成，每一层的输出都会对下一层的注意力机制产生影响。

  Prompt在进入transformer层后，在每个self-attention层都会让Prompt与class Prompt进行交互，从而改变class token的注意力分布 -> 改变下游层看到的上下文embedding -> 调整整个句子的语义。

  在自注意机制中 QKV 都是从(prompt, class)线性映射出来的, class token的更新也依赖与Prompt token。



**下阶段目标**：阅读BIP的论文和代码。



### MaPLe

**MaPLe: Multi-modal Prompt Learning**

现有prompt方式，仅关注单一分支(只关注文本或者只关注视觉)

提出MaPLe，为两个分支都设计prompt learning，强化视觉与文本表征的对齐。采用类似deep VPT的方式

同样针对泛化性，对新类别，新目标数据集，未见领域偏移泛化进行实验

<img src="./assets/image-20250926154133915.png" alt="image-20250926154133915" style="zoom:50%;" />

在文本和图像编码器上都加入可学习的prompt，并且两条分支的prompt通过一个耦合函数进行关联依赖，微调过程只调整context prompt和耦合函数

耦合函数的设计也是因为：图文输入是完全不同的，如果仅凭损失优化计算去拟合，计算代价相当大

language prompt上，采用类似deep VPT方式，在每个transformer层之前都引入新的可学习prompt，如果只引入一层则与CoOp一致；

vision prompt，也是采用类似deep VPT方式，但是vision prompt由language promp经过耦合函数投影获得；

耦合函数coupling function，为一个线性层，实现将维度$d_l$映射到另一个维度$d_v$

![image-20251028145838082](./assets/image-20251028145838082.png)

实验以CoCoOp为baseline，在相同的数据集上进行训练和各种泛化实验进行对比

消融实验：shallow设计，单视觉prompt，单语言prompt，视觉和语言prompt同时采用但不进行耦合

prompt深度(9) , prompt长度(过长会过拟合)，在分布偏移更大的数据集上表现会更优于CoCoOp

计算复杂度增加很少，参数量达到原来的2%



### PromptSRC

**Self-regulating Prompts: Foundational Model Adaptation without Forgetting** 

与MaPle同一个作者所发，由于maple是在few shot上训练，其deep结构的prompt以及图像和文本之间prompt的映射实际上产生了过拟合，因此本文通过正则化来解决过拟合的问题

针对模态对齐，从正则化角度进行研究

在prompt方法中，prompt的参数，即context embedding是在下游任务的loss上直接进行优化的(task-specific)，随着训练推进，embedding越来越偏向与训练集的分布和规律，忽略原本CLIP的更大分布，产生过拟合。因此针对prompt如何才能够**同时对task-specific和task无关进行学习**很关键

在设计时候对CLIP进行了关键改动，通过索引`i`记录当前transformer层数，然后插入可学习Prompt

<img src="./assets/image-20251110192508423.png" alt="image-20251110192508423" style="zoom:67%;" />

类似deep-VPT的设计

```python
def forward(self, x: torch.Tensor):
        if self.add_prompt:
            if not self.text_layer: # visual
                #去掉尾部的n_ctx_visual长度的tokens
                prefix = x[0:x.shape[0] - self.n_ctx_visual, :, :]
                visual_context = self.VPT_shallow.expand(x.shape[1], -1, -1).permute(1, 0, 2).half()
                x = torch.cat([prefix, visual_context], dim=0) # 再把可学习的部分给接上
            else:
                prefix = x[:1, :, :] # SOS or CLS
                suffix = x[1 + self.n_ctx_text:, :, :] # 后面的部分tokens
                textual_context = self.VPT_shallow.expand(x.shape[1], -1, -1).permute(1, 0, 2).half()
                x = torch.cat([prefix, textual_context, suffix], dim=0) # CLS + prompt learner + 原本后面部分

        x = x + self.attention(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x
```



提出**PromptSRC**，对prompt的自正则化框架，来引导prompt的学习，提高性能和泛化能力。

<img src="./assets/image-20251001142341252.png" alt="image-20251001142341252" style="zoom: 67%;" />

promptSRC通过三个方式同时对prompt进行调节：

1.最大化prompt和CLIP特征之间的agreement，施加明确的一致性约束(保留冻结的预训练CLIP本身的分布表达能力)。 具体来说将可学习的特征与预训练CLIP的特征施加约束即**损失函数**，以及原本的优化prompt对下游任务的损失函数相加获得的作为最终损失函数  **(简单说：图像经过图像编码器获得一个表征，拼接上可学习的prompt也得到一个表征，然后对这两个表征进行一致性约束)** 

**通过三个正则化去优化了MaPLe中文本编码器和图像编码器可学习prompt之间耦合函数带来的过拟合**

2.自集成调节，对prompt采用weighted prompt aggregation technique，权重从高斯分布中采样，即对于每一轮的visual prompt和text prompt都**按照高斯分布进行加权平均集成**(减少针对任务训练过程中越来越偏向于训练集的分布和规律, 因此初期和后期的权重都相对较低) (产生两个超参数即高斯分布的均值和标准差)

代码层面实现: 当运行**本个epoch的最后一个batch**时候，将**当前模型参数进行高斯加权**，然后与上一个epoch的模型**权重进行叠加**，更新模型权重。

<img src="./assets/image-20251106191526277.png" alt="image-20251106191526277" style="zoom:67%;" />

<img src="./assets/image-20251106204253166.png" alt="image-20251106204253166" style="zoom:67%;" />

3.为text部分设计多样化label，**为特定类别定义多个文本label**，即对文本进行augmentation，构建多个prompt模版，减少与图像模态的差异(视觉一个类别对应多个图像，文本则一个类别只对应一个label)。在训练阶段为prompt ensembling feature和prompt feature进行正则化，推理时仍为prompt feature  

在文本输入端进行增强的部分: 直接将类别套上了Prompt emsembling的模版, 然后取平均获得一个融合版的文本向量

<img src="./assets/image-20251104152049693-1762432995806-33.png" alt="image-20251104152049693" style="zoom: 67%;" />

实验：

从基础类别到新类别的泛化、few shot实验{1,2,4,8,16}、分布外数据集的领域泛化、跨数据集评估

baseline: CoOp,CoCoOp     benchmark: baseline中使用的11类data sets

model: ViT-B/16 CLIP

消融实验：对于第一个组件，测试了对learnable prompt feature和CLIP feature之间用不同损失函数施加一致性约束的性能，结果标明L1效果优于余弦相似度和MSE；对第二个组件，测试了施加权重分配的性能；

对训练和推理计算成本也进行了分析，参数量相比maple减少77倍，训练时间低于CoCoOp，推理阶段相比单独的VL prompt结构没有额外开销

学习率相比于maple有所调整



### PLOT

**PLOT: PROMPT LEARNING WITH OPTIMAL TRANSPORT FOR VISION-LANGUAGE MODELS**

同样是针对文本端，利用LLM的知识来增强文本端的输入，提出使用多个Prompt进行学习。但问题在于Prompt与视觉特征之间的匹配，提出对应算法进行解决。

问题：文中提到直接使用多个Prompt分别与图像特征对齐，会导致Prompt特征收敛到同一个点：每个Prompt与图像特征对齐过程中，只是接近，而不是关注差异，导致最终都靠拢到一起

PLOT：用于实现局部视觉特征与多个文本提示词的对齐，实现细粒度的模态对齐。(传统欧氏距离是计算全局特征与Prompt的距离)

将OT(最优运输算法)应用于多个Prompt与图像特征之间，使用OT算法替换原来图文特征之间的余弦相似度计算

分为两阶段：1.用sinkhorn算法来优化计算过程 2.固定算法参数进行反向传播更新Prompt

消融实验：

PLOT计算方法相比余弦相似度使得推理速度有所下降，训练时间也有所延长

仅针对few shot的方法起作用



| method (shot=16) | lr     | epoch | n_ctx | prompt_depth | batch_size | optim | others                                                       |
| ---------------- | ------ | ----- | ----- | ------------ | ---------- | ----- | ------------------------------------------------------------ |
|                  |        |       |       |              |            |       |                                                              |
| maple            | 0.0035 | 5     | 2     | 9            | 4          | sgd   |                                                              |
| promptSRC        | 0.0025 | 20    | 4     | 9            | 4          | sgd   | 损失项权重： $L_{SCL-image}=10$ $L_{SCL-text}=25$ 高斯加权集成: |
| TCP              | 0.002  | 50    | 4     |              | 32         | adam  |                                                              |



### CoPrompt

**CONSISTENCY-GUIDED PROMPT LEARNING FOR VISION-LANGUAGE MODELS**

结合CLIP-Adapter，maple，PromptSRC，强化了正则效果

对**可训练模型**和**预训练模型**的预测结果做一致性约束。两个组件：对扰动输入(即做数据增强)进行一致性约束；Adapter和Prompt两种方法的融合)

与KgCoOp的思路有相似之处，为了保证原有模型的泛化能力，对经过微调的输出和原本的输出做一致性约束

涉及知识蒸馏概念，让预训练模型通过一致性约束，实现冻结编码器到可学习编码器的知识蒸馏

文本分支，也利用LLM作为外部知识，让LLM对原有文本生成更加详细的描述，将其输出与可训练端进行一致性约束；图像分支，则通过图形增强，将其输出与可训练端进行一致性约束

<img src="./assets/image-20251024141414647.png" alt="image-20251024141414647" style="zoom:67%;" />

设计与**PromptSRC的多模态方法**十分相似，文章强调不同点在于：加入了Adapter模块，需要同时对Adapter进行调参；在文本端所做的数据增强由LLM生成，比起手工设计要更加细致；使用的损失计算方法也不同

创新点在于同时在文本和图像的可训练端的编码器输出位置都加上了Adapter(文中说先前研究中在双侧加上Adapter效果较差，但事实效果好)，Adapter的设计与常用的相同(两层线性层+非线性激活)

实验：在跨数据集上的表现比PromptSRC优秀，领域泛化上数据稍低

消融实验

![image-20251031161852023](./assets/image-20251031161852023.png)





### PAmethod

**Prompt-based Adaptation in Large-scale Vision Models: A Survey**

给VPT和VP做区分, 将现有方法按照可学习提示、生成式提示与非可学习提示三类，以及从语义空间还是像素空间注入进行区分，将其统一为PA(Prompt-based Adapter)方法内的两大模块

VP针对像素级做Prompt。而VPT作用与内部的token和特征序列

探讨了这两种范式的差异，应用场景，基础理论的分析和问题挑战

| 方法                                                         | 分类                                             | 主要应用领域/场景        |
| ------------------------------------------------------------ | ------------------------------------------------ | ------------------------ |
| **AttrVP** (Chen & Wang, 2025)                               | **VP – Learnable / Pixel**                       | 通用视觉（参数高效适配） |
| **LøR-VP** (Jin et al., 2025)                                | **VP – Learnable / Pixel**                       | 通用视觉（参数高效适配） |
| **AdaPrompt** (Le et al., 2025)                              | **VPT – Learnable / Token**                      | 通用视觉（跨任务）       |
| **SG-VPT** (Ren et al., 2025)                                | **VPT – Learnable / Token**                      | 通用视觉；注重跨任务泛化 |
| **DVPT** (He et al., 2025a)                                  | **VPT – Generated / Token**                      | 医学分析                 |
| **DDFP** (Yin et al., 2025)                                  | **VP – Learnable / Pixel**                       | 医学图像分割             |
| **BiomedDPT** (Peng et al., 2025)                            | **VPT**                                          | 生物医学图像分类         |
| **OT-VP** (Zhang et al., 2025d)                              | **VP – Learnable / Pixel**                       | TTA                      |
| **ZoRI** (Huang et al., 2025a）                              | **VP – Fixed（输入线索）/ Pixel**                | 遥感实例分割             |
| **DynaPrompt** (Xiao et al., 2025f)                          | **VPT – Generated / Token**                      | 测试时自适应             |
| **DPCore** (Zhang et al., 2025e)                             | **VPT – Learnable / Token**（“动态提示核心集”）  | 持续测试时适应           |
| **Image-aware Dynamic Prompts for Anomaly Segmentation** (Zhang et al., 2025c) | **VPT – Learnable / Token**（“dynamic prompts”） | 异常分割                 |
| **IA Instance** (Li et al., 2025f)                           | VP-Learned                                       | 遥感实例分割             |
| **RLita**（Zhang et al., 2025b).                             | VPT                                              | 遥感图文对齐             |
| **Layerlink** （Zhu et al., 2025).                           | VPT                                              | 遥感; VLM                |
| **SPT** (Yang et al., 2025)                                  | VP-Learned                                       | 异常分割                 |
| **SAID** (Huang et al., 2025b)                               | VP-Learned                                       | 异常分割                 |
| **ClipSAM** (Li et al., 2025e)                               | VP-Generated                                     | 异常分割                 |
| **IAPAS** (Zhang et al., 2025c)                              | VP-Generated                                     | Industrial               |
| **UWSAM** (Li et al., 2025c)                                 | VP-Generated                                     | Underwater               |
| **GAPrompt** Ai et al. (2025)                                | VPT                                              | 点云                     |
| **PointLoRA** Wang et al. (2025a)                            | VP                                               | 点云                     |
| **RoadBench** Xiao et al. (2026; 2025a)                      | VP                                               | 自动驾驶                 |
| **PF3Det **(Li et al., 2025d).                               | VPT                                              | 三维检测 工业            |
| **MagicID**(Li et al., 2023a; 2025b).                        |                                                  | 压缩视频微调             |
| **TP-CLIP** (Gowda et al., 2025).                            | VP-generated CLIP                                | 视频动作识别             |
| **STOP** (Liu et al., 2025d).                                | VPT                                              | 开放领域视频理解         |
| **TEST-V** (Yan et al., 2025).                               |                                                  | 零样本视频分类           |
| **SEA-Net** (He et al., 2025b).                              | VPT                                              | 水下语义分割             |
| **OSLOPROMPT** Gupta et al., 2025)                           |                                                  | 领域偏移情景下健壮性     |



| 标题                                                         | 分类              | 应用                             |
| ------------------------------------------------------------ | ----------------- | -------------------------------- |
| Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want. | VP                | visual prompt引导MLLM            |
| Exploring the Transferability of Visual Prompting for Multimodal Large Language Models | VP                | visual prompt引导MLLM            |
| A remote sensing change detection network using visualprompt enhanced CLIP | VPT-enhanced CLIP | 时序推理任务                     |
| **ClipSAM**: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation | VP-Generated      | 异常分割                         |
| @ **BIOMED-DPT**: DUAL MODALITY PROMPT TUNING FOR BIOMEDICAL VISION-LANGUAGE MODELS | VPT               | 生物医学图像分类                 |
| @ **Cibr**: Cross-modal information bottleneck regularization for robust clip generalization. | VPT-enhanced      | CLIP通用的跨模态正则化框架       |
| RoadBench: A Vision-Language Foundation Model and Benchmark for Road Damage Understanding **(RoadCLIP)** | VP                | 自动驾驶; CLIP编码器融入先验知识 |
| **BEV-CLIP**: Multi-modal BEV Retrieval Methodology for Complex Scene in Autonomous Driving | VPT-generated     | 自动驾驶; LoRA微调编码器适配领域 |





### TCP

**TCP:Textual-based Class-aware Prompt tuning for Visual-Language Model**

**KgCoOp上的改进**，还结合了PRO的一部分设计

针对文本端，采用先验知识方法，设计了一个**TKE**模块，将文本知识映射到 class-aware prompts中，在将这些Prompt集成到文本编码器

简单说就是：将手工设计文本和class 1.进行embedding和Prompt learner放在一起 2.在自己经过原文本编码器然后通过一个类似Adapter的模块，插入到可学习端中的中间层继续训练 3.最后再对两个端的输出进行对比损失

<img src="./assets/image-20251109135124408.png" alt="image-20251109135124408" style="zoom: 80%;" />

1.随机初始化ctx (n_cls, n_ctx, ctx_dim) -> prompt = pre(sos) + ctx + suff(cls, eos) -> posit_embed -> x

2.手工设计模版+class -> clip_model_.encode_text() -> TKE ->class prompt

3.x -> transformer(1-7) -> transformer8 : class prompt 替换掉了 中间的ctx 

<img src="./assets/image-20251109195032103.png" alt="image-20251109195032103" style="zoom:67%;" />

<img src="./assets/image-20251109194924873.png" alt="image-20251109194924873" style="zoom:67%;" />

消融实验：

将TKE模块插入到现有主流方法中，性能都呈现提升

随机初始化的性能在TCP中优于手工设计模版初始化

在补充的实验部分，还探讨了在插入class-aware prompt时候权重w的大小设置，设置为1直接进行替换效果最优

